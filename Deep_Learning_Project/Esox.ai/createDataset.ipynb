{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               CheckIn            CheckOut      Class  HoursWorked MemberID\n",
      "0  2017-01-02 09:15:00 2017-01-02 17:40:00   punctual     08:25:00     Mike\n",
      "1  2017-01-03 09:11:00 2017-01-03 17:53:00   punctual     08:42:00     Mike\n",
      "2  2017-01-04 09:07:00 2017-01-04 17:44:00   punctual     08:37:00     Mike\n",
      "3  2017-01-05 09:12:00 2017-01-05 17:26:00   punctual     08:14:00     Mike\n",
      "4  2017-01-06 09:17:00 2017-01-06 17:48:00   punctual     08:31:00     Mike\n",
      "5  2017-01-09 09:15:00 2017-01-09 17:50:00   punctual     08:35:00     Mike\n",
      "6  2017-01-10 09:21:00 2017-01-10 17:30:00   punctual     08:09:00     Mike\n",
      "7  2017-01-11 09:26:00 2017-01-11 17:47:00   punctual     08:21:00     Mike\n",
      "8  2017-01-12 09:30:00 2017-01-12 17:38:00   punctual     08:08:00     Mike\n",
      "9  2017-01-13 09:07:00 2017-01-13 17:58:00   punctual     08:51:00     Mike\n",
      "10 2017-01-16 09:09:00 2017-01-16 17:47:00   punctual     08:38:00     Mike\n",
      "11 2017-01-17 09:20:00 2017-01-17 17:23:00   punctual     08:03:00     Mike\n",
      "12 2017-01-18 09:14:00 2017-01-18 17:47:00   punctual     08:33:00     Mike\n",
      "13 2017-01-19 09:26:00 2017-01-19 18:02:00   punctual     08:36:00     Mike\n",
      "14 2017-01-20 09:13:00 2017-01-20 17:32:00   punctual     08:19:00     Mike\n",
      "15 2017-01-23 09:18:00 2017-01-23 18:09:00   punctual     08:51:00     Mike\n",
      "16 2017-01-24 09:06:00 2017-01-24 17:37:00   punctual     08:31:00     Mike\n",
      "17 2017-01-25 09:19:00 2017-01-25 17:36:00   punctual     08:17:00     Mike\n",
      "18 2017-01-26 09:06:00 2017-01-26 17:26:00   punctual     08:20:00     Mike\n",
      "19 2017-01-27 09:37:00 2017-01-27 17:39:00   punctual     08:02:00     Mike\n",
      "20 2017-01-30 09:23:00 2017-01-30 17:44:00   punctual     08:21:00     Mike\n",
      "21 2017-01-31 09:04:00 2017-01-31 17:49:00   punctual     08:45:00     Mike\n",
      "22 2017-02-01 09:25:00 2017-02-01 17:57:00   punctual     08:32:00     Mike\n",
      "23 2017-02-02 09:15:00 2017-02-02 17:48:00   punctual     08:33:00     Mike\n",
      "24 2017-02-03 09:19:00 2017-02-03 17:28:00   punctual     08:09:00     Mike\n",
      "25 2017-01-02 09:14:00 2017-01-02 16:37:00  hw-eratic     07:23:00      Tom\n",
      "26 2017-01-03 09:12:00 2017-01-03 17:50:00  hw-eratic     08:38:00      Tom\n",
      "27 2017-01-04 09:36:00 2017-01-04 15:46:00  hw-eratic     06:10:00      Tom\n",
      "28 2017-01-05 09:31:00 2017-01-05 17:14:00  hw-eratic     07:43:00      Tom\n",
      "29 2017-01-06 09:08:00 2017-01-06 21:42:00  hw-eratic     12:34:00      Tom\n",
      "..                 ...                 ...        ...          ...      ...\n",
      "70 2017-01-30 10:00:00 2017-01-30 13:02:00   parttime     03:02:00       Al\n",
      "71 2017-01-31 10:21:00 2017-01-31 13:51:00   parttime     03:30:00       Al\n",
      "72 2017-02-01 09:39:00 2017-02-01 15:43:00   parttime     06:04:00       Al\n",
      "73 2017-02-02 09:58:00 2017-02-02 14:42:00   parttime     04:44:00       Al\n",
      "74 2017-02-03 10:10:00 2017-02-03 14:50:00   parttime     04:40:00       Al\n",
      "75 2017-01-02 12:28:00 2017-01-02 15:55:00        bad     03:27:00    Smith\n",
      "76 2017-01-03 12:13:00 2017-01-03 15:26:00        bad     03:13:00    Smith\n",
      "77 2017-01-04 12:14:00 2017-01-04 13:11:00        bad     00:57:00    Smith\n",
      "78 2017-01-05 12:23:00 2017-01-05 15:12:00        bad     02:49:00    Smith\n",
      "79 2017-01-06 12:40:00 2017-01-06 14:50:00        bad     02:10:00    Smith\n",
      "80 2017-01-09 12:13:00 2017-01-09 14:48:00        bad     02:35:00    Smith\n",
      "81 2017-01-10 12:10:00 2017-01-10 14:44:00        bad     02:34:00    Smith\n",
      "82 2017-01-11 12:18:00 2017-01-11 16:52:00        bad     04:34:00    Smith\n",
      "83 2017-01-12 12:15:00 2017-01-12 13:55:00        bad     01:40:00    Smith\n",
      "84 2017-01-13 12:21:00 2017-01-13 14:02:00        bad     01:41:00    Smith\n",
      "85 2017-01-16 12:17:00 2017-01-16 16:29:00        bad     04:12:00    Smith\n",
      "86 2017-01-17 12:28:00 2017-01-17 15:10:00        bad     02:42:00    Smith\n",
      "87 2017-01-18 12:18:00 2017-01-18 13:39:00        bad     01:21:00    Smith\n",
      "88 2017-01-19 12:11:00 2017-01-19 15:20:00        bad     03:09:00    Smith\n",
      "89 2017-01-20 12:24:00 2017-01-20 14:44:00        bad     02:20:00    Smith\n",
      "90 2017-01-23 12:28:00 2017-01-23 16:25:00        bad     03:57:00    Smith\n",
      "91 2017-01-24 12:30:00 2017-01-24 15:22:00        bad     02:52:00    Smith\n",
      "92 2017-01-25 12:23:00 2017-01-25 14:56:00        bad     02:33:00    Smith\n",
      "93 2017-01-26 12:30:00 2017-01-26 16:47:00        bad     04:17:00    Smith\n",
      "94 2017-01-27 12:31:00 2017-01-27 14:47:00        bad     02:16:00    Smith\n",
      "95 2017-01-30 12:29:00 2017-01-30 16:15:00        bad     03:46:00    Smith\n",
      "96 2017-01-31 12:08:00 2017-01-31 14:34:00        bad     02:26:00    Smith\n",
      "97 2017-02-01 12:30:00 2017-02-01 15:42:00        bad     03:12:00    Smith\n",
      "98 2017-02-02 12:24:00 2017-02-02 14:31:00        bad     02:07:00    Smith\n",
      "99 2017-02-03 12:23:00 2017-02-03 14:38:00        bad     02:15:00    Smith\n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pandas.tseries.offsets import *\n",
    "\n",
    "#http://pandas.pydata.org/pandas-docs/stable/timeseries.html#overview\n",
    "#https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html\n",
    "#http://pandas.pydata.org/pandas-docs/stable/timeseries.html#generating-ranges-of-timestamps\n",
    "#http://pandas.pydata.org/pandas-docs/stable/timeseries.html#dateoffset-objects\n",
    "#http://pandas.pydata.org/pandas-docs/stable/timeseries.html#timeseries-offset-aliases\n",
    "\n",
    "def getEmployeeData(startdatetimecheckin, startdatetimecheckout, count, employeename, label, randomCheckinMinsMean, randomCheckinMinsVariance, randomCheckoutMinsMean, randomCheckoutMinsVariance):\n",
    "    \"This function\"\n",
    "    nameColumn0 = pd.Series(np.array(employeename).repeat(count))\n",
    "\n",
    "    startDatetimeCheckinColumn1 = pd.Series(pd.date_range(startdatetimecheckin, periods=count, freq='B'))\n",
    "    randomMinsList = 10 * np.random.randn(count) + randomCheckinMinsMean\n",
    "    randomMinsSeries = pd.Series(randomMinsList, dtype=int)\n",
    "    for index, val in startDatetimeCheckinColumn1.iteritems():\n",
    "        startDatetimeCheckinColumn1[index] = startDatetimeCheckinColumn1[index] + Minute(randomMinsSeries[index])\n",
    "\n",
    "    startDatetimeCheckoutColumn2 = pd.Series(pd.date_range(startdatetimecheckout, periods=count, freq='B'))\n",
    "    randomMinsList2 = randomCheckoutMinsVariance * np.random.randn(count) + randomCheckoutMinsMean\n",
    "    randomMinsSeries2 = pd.Series(randomMinsList2, dtype=int)\n",
    "    for index, val in startDatetimeCheckoutColumn2.iteritems():\n",
    "        startDatetimeCheckoutColumn2[index] = startDatetimeCheckoutColumn2[index] + Minute(randomMinsSeries2[index])\n",
    "\n",
    "    labelColumn3 = pd.Series(np.array(label).repeat(count))\n",
    "\n",
    "    data = pd.DataFrame({'MemberID': nameColumn0, 'CheckIn': startDatetimeCheckinColumn1, 'CheckOut': startDatetimeCheckoutColumn2, 'HoursWorked': startDatetimeCheckoutColumn2 - startDatetimeCheckinColumn1, 'Class': labelColumn3})\n",
    "    #print(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "punctionalEmployeeData = getEmployeeData('2017-1-2 9:00:00', '2017-1-2 17:00:00', 25, 'Mike', 'punctual', 15, 10, 45, 10)\n",
    "hardWorkerEraticEmployeeData = getEmployeeData('2017-1-2 9:00:00', '2017-1-2 17:00:00', 25, 'Tom', 'hw-eratic', 15, 100, 60, 100)\n",
    "parttimerEmployeeData = getEmployeeData('2017-1-2 9:00:00', '2017-1-2 13:30:00', 25, 'Al', 'parttime', 60, 100, 60, 100)\n",
    "badEmployeeData = getEmployeeData('2017-1-2 9:00:00', '2017-1-2 17:00:00', 25, 'Smith', 'bad', 200, 100, -100, 60)\n",
    "learningData = pd.concat([punctionalEmployeeData, hardWorkerEraticEmployeeData, parttimerEmployeeData, badEmployeeData], ignore_index=True)\n",
    "print(learningData)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
